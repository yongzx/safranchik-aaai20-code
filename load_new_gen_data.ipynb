{
 "cells": [
  {
   "source": [
    "import pickle\n",
    "import json"
   ],
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T16:44:00.944279Z",
     "start_time": "2021-06-22T16:44:00.934770Z"
    }
   },
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15f80c48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T16:51:24.548628Z",
     "start_time": "2021-06-22T16:51:23.959760Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['tokens', 'tags', 'unary_marginals', 'vote_mask']"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "test_data = pickle.load(open(\"compare-gen-results/new-wiser/test_data.p\", \"rb\"))\n",
    "dev_data = pickle.load(open(\"compare-gen-results/new-wiser/dev_data.p\", \"rb\"))\n",
    "train_data = pickle.load(open(\"compare-gen-results/new-wiser/train_data_mv.p\", \"rb\"))\n",
    "list(train_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b3762e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "33\n188\n257\n374\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluate on tokenization\n",
    "\"\"\"\n",
    "\n",
    "# ### test data\n",
    "# new_test_data = {}\n",
    "# for i in range(len(test_data)):\n",
    "#     new_test_data[i] = [token.text for token in test_data[i]['tokens'].tokens]\n",
    "\n",
    "# with open(\"tmp/old_test_data.txt\") as rf:\n",
    "#     old_test_data = json.load(rf)\n",
    "\n",
    "# for i in range(len(test_data)):\n",
    "#     try:\n",
    "#         assert old_test_data[str(i)] == new_test_data[i]\n",
    "#     except:\n",
    "#         print(i)\n",
    "\n",
    "#         # 49: old: 'Na', '+', '/', 'K', '+', 'ATPase'  | new: 'Na', '+', '/K', '+', 'ATPase'\n",
    "\n",
    "# ### dev data\n",
    "# new_dev_data = {}\n",
    "# for i in range(len(dev_data)):\n",
    "#     new_dev_data[i] = [token.text for token in dev_data[i]['tokens'].tokens]\n",
    "\n",
    "# with open(\"tmp/old_dev_data.txt\") as rf:\n",
    "#     old_dev_data = json.load(rf)\n",
    "\n",
    "# for i in range(len(dev_data)):\n",
    "#     try:\n",
    "#         assert old_dev_data[str(i)] == new_dev_data[i]\n",
    "#     except:\n",
    "#         print(i)\n",
    "\n",
    "### train data\n",
    "new_train_data = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_train_data[i] = [token.text for token in train_data[i]['tokens'].tokens]\n",
    "\n",
    "with open(\"tmp/old_train_data.txt\") as rf:\n",
    "    old_train_data = json.load(rf)\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    try:\n",
    "        assert old_train_data[str(i)] == new_train_data[i]\n",
    "    except:\n",
    "        print(i)\n",
    "\n",
    "        # 33: '/', 'glucose' vs '/glucose'\n",
    "        # 188: '/', 'K' vs '/K'\n",
    "        # 257: '/', 'glucose' vs '/glucose'\n",
    "        # 374: '/', 'glucose' vs '/glucose'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nEvaluate on tags\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluate on tags\n",
    "\"\"\"\n",
    "\n",
    "# ### dev data\n",
    "# new_dev_data_tags = {}\n",
    "# for i in range(len(dev_data)):\n",
    "#     new_dev_data_tags[i] = dev_data[i]['tags'].labels\n",
    "\n",
    "# with open(\"tmp/old_dev_data_tags.txt\") as rf:\n",
    "#     old_dev_data_tags = json.load(rf)\n",
    "\n",
    "# for i in range(len(dev_data)):\n",
    "#     try:\n",
    "#         assert old_dev_data_tags[str(i)] == new_dev_data_tags[i]\n",
    "#     except:\n",
    "#         print(i)\n",
    "\n",
    "# ### train data\n",
    "# new_train_data_tags = {}\n",
    "# for i in range(len(train_data)):\n",
    "#     new_train_data_tags[i] = train_data[i]['tags'].labels\n",
    "\n",
    "# with open(\"tmp/old_train_data_tags.txt\") as rf:\n",
    "#     old_train_data_tags = json.load(rf)\n",
    "\n",
    "# for i in range(len(train_data)):\n",
    "#     try:\n",
    "#         assert old_train_data_tags[str(i)] == new_train_data_tags[i]\n",
    "#     except:\n",
    "#         print(i)\n",
    "\n",
    "#         # 33: '/', 'glucose' vs '/glucose'\n",
    "#         # 188: '/', 'K' vs '/K'\n",
    "#         # 257: '/', 'glucose' vs '/glucose'\n",
    "#         # 374: '/', 'glucose' vs '/glucose'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate on train_mv unary marginals\n",
    "\"\"\"\n",
    "old_train_data_pkl = pickle.load(open(\"tmp/old_train_data_unary_marginals.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\n",
      "4\n",
      "7\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "16\n",
      "19\n",
      "20\n",
      "21\n",
      "23\n",
      "24\n",
      "26\n",
      "27\n",
      "28\n",
      "30\n",
      "31\n",
      "33\n",
      "34\n",
      "36\n",
      "37\n",
      "38\n",
      "40\n",
      "41\n",
      "42\n",
      "44\n",
      "45\n",
      "47\n",
      "48\n",
      "50\n",
      "52\n",
      "53\n",
      "55\n",
      "57\n",
      "59\n",
      "62\n",
      "63\n",
      "64\n",
      "70\n",
      "73\n",
      "74\n",
      "76\n",
      "77\n",
      "84\n",
      "85\n",
      "88\n",
      "89\n",
      "90\n",
      "93\n",
      "96\n",
      "98\n",
      "115\n",
      "118\n",
      "120\n",
      "122\n",
      "126\n",
      "127\n",
      "128\n",
      "130\n",
      "131\n",
      "134\n",
      "136\n",
      "138\n",
      "140\n",
      "142\n",
      "143\n",
      "145\n",
      "149\n",
      "150\n",
      "151\n",
      "153\n",
      "154\n",
      "156\n",
      "166\n",
      "168\n",
      "171\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "178\n",
      "185\n",
      "187\n",
      "188\n",
      "191\n",
      "193\n",
      "194\n",
      "195\n",
      "199\n",
      "203\n",
      "205\n",
      "209\n",
      "210\n",
      "214\n",
      "217\n",
      "220\n",
      "221\n",
      "225\n",
      "227\n",
      "232\n",
      "237\n",
      "240\n",
      "242\n",
      "243\n",
      "244\n",
      "246\n",
      "250\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "260\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "273\n",
      "275\n",
      "277\n",
      "278\n",
      "281\n",
      "283\n",
      "286\n",
      "287\n",
      "289\n",
      "290\n",
      "294\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "303\n",
      "305\n",
      "306\n",
      "307\n",
      "320\n",
      "322\n",
      "323\n",
      "328\n",
      "330\n",
      "331\n",
      "335\n",
      "336\n",
      "340\n",
      "342\n",
      "343\n",
      "346\n",
      "347\n",
      "351\n",
      "352\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "359\n",
      "361\n",
      "363\n",
      "367\n",
      "370\n",
      "371\n",
      "374\n",
      "376\n",
      "377\n",
      "380\n",
      "381\n",
      "385\n",
      "387\n",
      "389\n",
      "394\n",
      "395\n",
      "396\n",
      "398\n",
      "399\n",
      "400\n",
      "406\n",
      "412\n",
      "417\n",
      "419\n",
      "422\n",
      "426\n",
      "428\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "448\n",
      "449\n",
      "450\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "458\n",
      "465\n",
      "467\n",
      "472\n",
      "473\n",
      "477\n",
      "478\n",
      "480\n",
      "482\n",
      "483\n",
      "484\n",
      "490\n",
      "492\n",
      "493\n",
      "496\n",
      "497\n",
      "499\n",
      "501\n",
      "504\n",
      "506\n",
      "507\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "516\n",
      "518\n",
      "521\n",
      "522\n",
      "526\n",
      "529\n",
      "530\n",
      "536\n",
      "539\n",
      "540\n",
      "542\n",
      "554\n",
      "555\n",
      "560\n",
      "564\n",
      "567\n",
      "568\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "591\n",
      "/Users/zhengxinyong/Desktop/esteban/env/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_data)):\n",
    "    try:\n",
    "        assert (np.argmax(train_data[i]['unary_marginals'].array, axis=1) == np.argmax(old_train_data_pkl[i], axis=1)).all()\n",
    "    except:\n",
    "        print(i)\n",
    "        # 2: (1, breast), (1, or), (1, ovarian), (1, cancer) ||| (1, breast), (0, or), (1, ovarian), (1, cancer)\n",
    "        # 4: (0, deficiency), (0, of), (0, the), (0, seventh), (0, component) ||| (1, deficiency), (0, of), (0, the), (1, seventh), (1, component)\n",
    "        # 7: (0, serositis), (0, or), (0, synovitis) ||| (1, serositis), (0, or), (1, synovitis)\n",
    "        # 9: (1, linked), (1, adrenoleukodystrophy) ||| (1, linked), (1, adrenoleukodystrophy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0, Absence), (0, of), (0, the), (0, seventh), (0, component), (0, of), (0, complement), (0, in), (0, a), (0, patient), (0, with), (0, chronic), (0, meningococcemia), (0, presenting), (0, as), (0, vasculitis), (0, .), (0, A), (0, previously), (0, healthy), (0, 40), (0, -), (0, year), (0, -), (0, old), (0, man), (0, presenting), (0, with), (1, fever), (0, ,), (1, arthritis), (0, ,), (0, and), (0, cutaneous), (0, vasculitis), (0, was), (0, found), (0, to), (0, have), (0, chronic), (0, meningococcemia), (0, .), (0, Evaluation), (0, of), (0, his), (0, complement), (0, system), (0, showed), (0, an), (0, absence), (0, of), (0, functional), (0, and), (0, antigenic), (0, C7), (0, ,), (0, compatible), (0, with), (0, a), (0, complete), (0, deficiency), (0, of), (0, the), (0, seventh), (0, component), (0, of), (0, complement), (0, .), (0, Study), (0, of), (0, the), (0, patients), (0, family), (0, spanning), (0, four), (0, generations), (0, showed), (0, heterozygous), (0, deficiency), (0, of), (0, C7), (0, in), (0, five), (0, members), (0, .), (0, Chronic), (0, neisserial), (0, infection), (0, can), (0, be), (0, associated), (0, with), (1, C7), (1, deficiency), (0, and), (0, must), (0, be), (0, distinguished), (0, from), (0, other), (0, causes), (0, of), (0, cutaneous), (0, vasculitis), (0, ..)]\n====================================================================================================\n[(0, Absence), (0, of), (0, the), (0, seventh), (0, component), (0, of), (0, complement), (0, in), (0, a), (0, patient), (0, with), (0, chronic), (1, meningococcemia), (0, presenting), (0, as), (1, vasculitis), (0, .), (0, A), (0, previously), (0, healthy), (0, 40), (0, -), (0, year), (0, -), (0, old), (0, man), (0, presenting), (0, with), (1, fever), (0, ,), (1, arthritis), (0, ,), (0, and), (0, cutaneous), (1, vasculitis), (0, was), (0, found), (0, to), (0, have), (0, chronic), (1, meningococcemia), (0, .), (0, Evaluation), (0, of), (0, his), (0, complement), (0, system), (0, showed), (0, an), (0, absence), (0, of), (0, functional), (0, and), (0, antigenic), (0, C7), (0, ,), (0, compatible), (0, with), (0, a), (0, complete), (1, deficiency), (0, of), (0, the), (1, seventh), (1, component), (0, of), (0, complement), (0, .), (0, Study), (0, of), (0, the), (0, patients), (0, family), (0, spanning), (0, four), (0, generations), (0, showed), (0, heterozygous), (1, deficiency), (0, of), (1, C7), (0, in), (0, five), (0, members), (0, .), (0, Chronic), (0, neisserial), (0, infection), (0, can), (0, be), (0, associated), (0, with), (1, C7), (1, deficiency), (0, and), (0, must), (0, be), (0, distinguished), (0, from), (0, other), (0, causes), (0, of), (0, cutaneous), (1, vasculitis), (0, ..)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "i = 10\n",
    "new_argmax = np.argmax(train_data[i]['unary_marginals'].array, axis=1)\n",
    "old_argmax = np.argmax(old_train_data_pkl[i], axis=1)\n",
    "print(list(zip(new_argmax, train_data[i]['tokens'])))\n",
    "print(\"=\"*100)\n",
    "print(list(zip(old_argmax, train_data[i]['tokens'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(new_argmax)):\n",
    "    if not (new_argmax == old_argmax)[j]:\n",
    "        print(j)\n",
    "list(zip(old_argmax, train_data[i]['tokens']))[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, serositis)"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 132 is out of bounds for axis 0 with size 105",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-74b9530e7a7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mold_train_data_pkl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m132\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 132 is out of bounds for axis 0 with size 105"
     ]
    }
   ],
   "source": [
    "old_train_data_pkl[i][132]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'3.0.6'"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "source": [
    "# LFs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['tokens', 'tags', 'WISER_LABELS', 'WISER_LINKS']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "old_ncbi_docs = pickle.load(open(\"tmp/old_ncbi_docs.p\", 'rb'))\n",
    "new_ncbi_docs = pickle.load(open(\"tmp/new_ncbi_docs.p\", 'rb'))\n",
    "new_ncbi_docs_allen = pickle.load(open(\"tmp/new_ncbi_docs_allennlp.p\", 'rb'))\n",
    "\n",
    "print(list(old_ncbi_docs[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "wiser_labels_matched = Counter()\n",
    "wiser_links_matched = Counter()\n",
    "count = 0\n",
    "\n",
    "for i in range(len(old_ncbi_docs)):\n",
    "    try:\n",
    "        if new_ncbi_docs[i]['tokens'] == old_ncbi_docs[i]['tokens']:  # only five unmatched\n",
    "            count += 1\n",
    "            for LF in new_ncbi_docs[i]['WISER_LABELS'].keys():\n",
    "                if new_ncbi_docs[i]['WISER_LABELS'][LF] == old_ncbi_docs[i]['WISER_LABELS'][LF]:\n",
    "                    wiser_labels_matched[LF] += 1 \n",
    "            for LR in new_ncbi_docs[i]['WISER_LINKS'].keys():\n",
    "                if new_ncbi_docs[i]['WISER_LINKS'][LR] == old_ncbi_docs[i]['WISER_LINKS'][LR]:\n",
    "                    wiser_links_matched[LR] += 1 \n",
    "    except Exception as e:\n",
    "        print(i, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üòï Matching Analysis of LFs: ======\n[Labeling Function] CoreDictionaryUncased: 100.00 %\n[Labeling Function] CoreDictionaryExact: 100.00 %\n[Labeling Function] CancerLike: 100.00 %\n[Labeling Function] CommonSuffixes: 99.36 %\n[Labeling Function] Deficiency: 94.14 %\n[Labeling Function] Disorder: 97.07 %\n[Labeling Function] Lesion: 99.62 %\n[Labeling Function] Syndrome: 97.20 %\n[Labeling Function] BodyTerms: 99.87 %\n[Labeling Function] StopWords: 76.31 %\n[Labeling Function] Punctuation: 100.00 %\n[Linking Rule] PossessivePhrase: 100.00 %\n[Linking Rule] HyphenatedPhrase: 100.00 %\n[Linking Rule] CommonBigram: 100.00 %\n[Linking Rule] ExtractedPhrase: 100.00 %\n"
     ]
    }
   ],
   "source": [
    "print(\"üòï Matching Analysis of LFs: ======\")\n",
    "\n",
    "for LF in wiser_labels_matched.keys():\n",
    "    print(f\"[Labeling Function] {LF}: {wiser_labels_matched[LF] / count * 100:.2f} %\")\n",
    "for LR in wiser_links_matched.keys():\n",
    "    print(f\"[Linking Rule] {LR}: {wiser_links_matched[LR] / count * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Doc: 6 Token: 144 A O ABS\n",
      "Doc: 6 Token: 280 its O ABS\n",
      "Doc: 6 Token: 357 A O ABS\n",
      "Doc: 11 Token: 102 A ABS O\n",
      "Doc: 11 Token: 122 A ABS O\n",
      "Doc: 19 Token: 151 I ABS O\n",
      "Doc: 20 Token: 297 its O ABS\n",
      "Doc: 22 Token: 205 i ABS O\n",
      "Doc: 34 Token: 23 A ABS O\n",
      "Doc: 34 Token: 50 i ABS O\n",
      "Doc: 35 Token: 67 A ABS O\n",
      "Doc: 35 Token: 191 its O ABS\n",
      "Doc: 36 Token: 178 its O ABS\n",
      "Doc: 41 Token: 32 its O ABS\n",
      "Doc: 44 Token: 6 its O ABS\n",
      "Doc: 53 Token: 262 its O ABS\n",
      "Doc: 55 Token: 97 its O ABS\n",
      "Doc: 57 Token: 271 its O ABS\n",
      "Doc: 60 Token: 224 Its O ABS\n",
      "Doc: 65 Token: 65 its O ABS\n",
      "Doc: 70 Token: 54 A ABS O\n",
      "Doc: 72 Token: 94 its O ABS\n",
      "Doc: 76 Token: 210 I O ABS\n",
      "Doc: 83 Token: 140 its O ABS\n",
      "Doc: 86 Token: 40 its O ABS\n",
      "Doc: 86 Token: 191 its O ABS\n",
      "Doc: 95 Token: 16 its O ABS\n",
      "Doc: 96 Token: 125 A ABS O\n",
      "Doc: 98 Token: 89 its O ABS\n",
      "Doc: 99 Token: 123 WHO O ABS\n",
      "Doc: 100 Token: 54 WHO O ABS\n",
      "Doc: 101 Token: 33 WAS O ABS\n",
      "Doc: 109 Token: 270 its O ABS\n",
      "Doc: 113 Token: 175 A O ABS\n",
      "Doc: 118 Token: 43 WAS O ABS\n",
      "Doc: 118 Token: 88 A ABS O\n",
      "Doc: 122 Token: 147 its O ABS\n",
      "Doc: 125 Token: 112 A ABS O\n",
      "Doc: 125 Token: 166 its O ABS\n",
      "Doc: 131 Token: 109 its O ABS\n",
      "Doc: 135 Token: 9 A O ABS\n",
      "Doc: 135 Token: 259 A O ABS\n",
      "Doc: 136 Token: 46 its O ABS\n",
      "Doc: 136 Token: 210 its O ABS\n",
      "Doc: 139 Token: 23 A ABS O\n",
      "Doc: 145 Token: 129 A ABS O\n",
      "Doc: 147 Token: 112 its O ABS\n",
      "Doc: 148 Token: 225 its O ABS\n",
      "Doc: 150 Token: 175 I ABS O\n",
      "Doc: 150 Token: 225 I ABS O\n",
      "Doc: 150 Token: 328 I O ABS\n",
      "Doc: 150 Token: 332 I ABS O\n",
      "Doc: 150 Token: 345 I O ABS\n",
      "Doc: 150 Token: 398 I ABS O\n",
      "Doc: 150 Token: 400 A ABS O\n",
      "Doc: 150 Token: 415 I ABS O\n",
      "Doc: 150 Token: 417 A ABS O\n",
      "Doc: 151 Token: 128 A O ABS\n",
      "Doc: 156 Token: 52 its O ABS\n",
      "Doc: 158 Token: 47 its O ABS\n",
      "Doc: 167 Token: 103 its O ABS\n",
      "Doc: 167 Token: 257 its O ABS\n",
      "Doc: 169 Token: 120 A ABS O\n",
      "Doc: 169 Token: 237 A ABS O\n",
      "Doc: 172 Token: 45 its O ABS\n",
      "Doc: 174 Token: 206 A ABS O\n",
      "Doc: 177 Token: 181 its O ABS\n",
      "Doc: 180 Token: 187 having ABS O\n",
      "Doc: 181 Token: 172 its O ABS\n",
      "Doc: 197 Token: 139 A ABS O\n",
      "Doc: 214 Token: 55 Its O ABS\n",
      "Doc: 214 Token: 178 its O ABS\n",
      "Doc: 216 Token: 230 A ABS O\n",
      "Doc: 217 Token: 97 its O ABS\n",
      "Doc: 228 Token: 60 its O ABS\n",
      "Doc: 229 Token: 85 A ABS O\n",
      "Doc: 229 Token: 192 A ABS O\n",
      "Doc: 231 Token: 9 its O ABS\n",
      "Doc: 233 Token: 158 its O ABS\n",
      "Doc: 248 Token: 71 its O ABS\n",
      "Doc: 253 Token: 192 i ABS O\n",
      "Doc: 262 Token: 46 its O ABS\n",
      "Doc: 267 Token: 79 A ABS O\n",
      "Doc: 267 Token: 273 its O ABS\n",
      "Doc: 270 Token: 16 A ABS O\n",
      "Doc: 270 Token: 44 A O ABS\n",
      "Doc: 270 Token: 92 A O ABS\n",
      "Doc: 270 Token: 129 A O ABS\n",
      "Doc: 271 Token: 155 its O ABS\n",
      "Doc: 274 Token: 29 WAS O ABS\n",
      "Doc: 274 Token: 135 WAS O ABS\n",
      "Doc: 274 Token: 161 WAS O ABS\n",
      "Doc: 277 Token: 250 A ABS O\n",
      "Doc: 281 Token: 239 its O ABS\n",
      "Doc: 283 Token: 117 having ABS O\n",
      "Doc: 285 Token: 198 A O ABS\n",
      "Doc: 285 Token: 269 A O ABS\n",
      "Doc: 285 Token: 339 A O ABS\n",
      "Doc: 285 Token: 389 A O ABS\n",
      "Doc: 285 Token: 450 A O ABS\n",
      "Doc: 287 Token: 8 its O ABS\n",
      "Doc: 287 Token: 74 its O ABS\n",
      "Doc: 289 Token: 48 A ABS O\n",
      "Doc: 289 Token: 152 A ABS O\n",
      "Doc: 296 Token: 223 A ABS O\n",
      "Doc: 297 Token: 19 A ABS O\n",
      "Doc: 297 Token: 72 A ABS O\n",
      "Doc: 305 Token: 47 Its O ABS\n",
      "Doc: 313 Token: 27 A ABS O\n",
      "Doc: 313 Token: 94 A ABS O\n",
      "Doc: 313 Token: 108 A ABS O\n",
      "Doc: 313 Token: 121 A ABS O\n",
      "Doc: 316 Token: 122 its O ABS\n",
      "Doc: 316 Token: 127 its O ABS\n",
      "Doc: 320 Token: 250 Am ABS O\n",
      "Doc: 323 Token: 41 its O ABS\n",
      "Doc: 325 Token: 34 its O ABS\n",
      "Doc: 326 Token: 290 its O ABS\n",
      "Doc: 329 Token: 90 A ABS O\n",
      "Doc: 340 Token: 17 its O ABS\n",
      "Doc: 347 Token: 119 its O ABS\n",
      "Doc: 349 Token: 151 its O ABS\n",
      "Doc: 359 Token: 198 having ABS O\n",
      "Doc: 366 Token: 7 its O ABS\n",
      "Doc: 366 Token: 29 its O ABS\n",
      "Doc: 368 Token: 113 A ABS O\n",
      "Doc: 370 Token: 299 its O ABS\n",
      "Doc: 377 Token: 17 A ABS O\n",
      "Doc: 377 Token: 49 A O ABS\n",
      "Doc: 383 Token: 290 its O ABS\n",
      "Doc: 389 Token: 98 WAS O ABS\n",
      "Doc: 389 Token: 150 WAS O ABS\n",
      "Doc: 389 Token: 202 WAS O ABS\n",
      "Doc: 389 Token: 220 WAS O ABS\n",
      "Doc: 389 Token: 252 WAS O ABS\n",
      "Doc: 391 Token: 40 A ABS O\n",
      "Doc: 391 Token: 137 A ABS O\n",
      "Doc: 391 Token: 171 WAS O ABS\n",
      "Doc: 410 Token: 52 A ABS O\n",
      "Doc: 410 Token: 77 A ABS O\n",
      "Doc: 410 Token: 305 Am O ABS\n",
      "Doc: 411 Token: 144 its O ABS\n",
      "Doc: 417 Token: 243 A O ABS\n",
      "Doc: 418 Token: 29 WAS O ABS\n",
      "Doc: 421 Token: 16 its O ABS\n",
      "Doc: 421 Token: 167 its O ABS\n",
      "Doc: 428 Token: 148 its O ABS\n",
      "Doc: 433 Token: 102 its O ABS\n",
      "Doc: 438 Token: 51 A ABS O\n",
      "Doc: 438 Token: 64 A ABS O\n",
      "Doc: 438 Token: 78 A ABS O\n",
      "Doc: 452 Token: 22 its O ABS\n",
      "Doc: 456 Token: 107 AS O ABS\n",
      "Doc: 456 Token: 147 AS O ABS\n",
      "Doc: 464 Token: 26 WAS O ABS\n",
      "Doc: 464 Token: 53 WAS O ABS\n",
      "Doc: 464 Token: 128 WAS O ABS\n",
      "Doc: 464 Token: 241 WAS O ABS\n",
      "Doc: 466 Token: 143 its O ABS\n",
      "Doc: 467 Token: 86 i ABS O\n",
      "Doc: 472 Token: 27 A ABS O\n",
      "Doc: 472 Token: 49 A ABS O\n",
      "Doc: 472 Token: 108 A O ABS\n",
      "Doc: 472 Token: 191 A O ABS\n",
      "Doc: 472 Token: 231 A O ABS\n",
      "Doc: 472 Token: 330 A O ABS\n",
      "Doc: 473 Token: 84 its O ABS\n",
      "Doc: 478 Token: 236 i ABS O\n",
      "Doc: 481 Token: 253 its O ABS\n",
      "Doc: 482 Token: 85 its O ABS\n",
      "Doc: 482 Token: 138 AS O ABS\n",
      "Doc: 486 Token: 197 its O ABS\n",
      "Doc: 487 Token: 10 its O ABS\n",
      "Doc: 488 Token: 66 A O ABS\n",
      "Doc: 492 Token: 5 its O ABS\n",
      "Doc: 492 Token: 151 its O ABS\n",
      "Doc: 492 Token: 273 its O ABS\n",
      "Doc: 494 Token: 152 its O ABS\n",
      "Doc: 497 Token: 147 its O ABS\n",
      "Doc: 499 Token: 114 A ABS O\n",
      "Doc: 505 Token: 122 its O ABS\n",
      "Doc: 512 Token: 199 its O ABS\n",
      "Doc: 515 Token: 316 i ABS O\n",
      "Doc: 518 Token: 11 A O ABS\n",
      "Doc: 518 Token: 23 A ABS O\n",
      "Doc: 518 Token: 206 A ABS O\n",
      "Doc: 518 Token: 316 A O ABS\n",
      "Doc: 522 Token: 272 its O ABS\n",
      "Doc: 532 Token: 27 WAS O ABS\n",
      "Doc: 539 Token: 266 A ABS O\n",
      "Doc: 541 Token: 51 its O ABS\n",
      "Doc: 545 Token: 162 WAS O ABS\n",
      "Doc: 557 Token: 92 its O ABS\n",
      "Doc: 559 Token: 203 its O ABS\n",
      "Doc: 560 Token: 36 A ABS O\n",
      "Doc: 560 Token: 163 A O ABS\n",
      "Doc: 560 Token: 225 A O ABS\n",
      "Doc: 561 Token: 39 A O ABS\n",
      "Doc: 564 Token: 68 A ABS O\n",
      "Doc: 572 Token: 168 A ABS O\n",
      "Doc: 573 Token: 40 its O ABS\n",
      "Doc: 575 Token: 64 its O ABS\n",
      "Doc: 579 Token: 284 its O ABS\n",
      "Doc: 582 Token: 249 A ABS O\n",
      "Doc: 583 Token: 98 WHO ABS O\n",
      "Doc: 586 Token: 11 its O ABS\n",
      "Doc: 592 Token: 162 A ABS O\n",
      "Doc: 604 Token: 76 I O ABS\n",
      "Doc: 604 Token: 190 A ABS O\n",
      "Doc: 609 Token: 231 WAS O ABS\n",
      "Doc: 610 Token: 33 A ABS O\n",
      "Doc: 611 Token: 9 its O ABS\n",
      "Doc: 611 Token: 266 its O ABS\n",
      "Doc: 616 Token: 120 its O ABS\n",
      "Doc: 626 Token: 26 its O ABS\n",
      "Doc: 626 Token: 109 Its O ABS\n",
      "Doc: 639 Token: 74 its O ABS\n",
      "Doc: 641 Token: 161 its O ABS\n",
      "Doc: 644 Token: 275 its O ABS\n",
      "Doc: 644 Token: 285 its O ABS\n",
      "Doc: 648 Token: 295 its O ABS\n",
      "Doc: 655 Token: 80 its O ABS\n",
      "Doc: 660 Token: 132 A O ABS\n",
      "Doc: 664 Token: 164 WAS O ABS\n",
      "Doc: 668 Token: 68 its O ABS\n",
      "Doc: 673 Token: 142 NO ABS O\n",
      "Doc: 673 Token: 170 NO ABS O\n",
      "Doc: 673 Token: 175 I ABS O\n",
      "Doc: 677 Token: 346 i ABS O\n",
      "Doc: 679 Token: 29 A ABS O\n",
      "Doc: 679 Token: 112 A O ABS\n",
      "Doc: 679 Token: 127 A O ABS\n",
      "Doc: 679 Token: 179 A O ABS\n",
      "Doc: 685 Token: 78 its O ABS\n",
      "Doc: 689 Token: 25 A ABS O\n",
      "Doc: 689 Token: 57 A O ABS\n",
      "Doc: 689 Token: 203 A O ABS\n",
      "Doc: 690 Token: 20 A ABS O\n",
      "Doc: 690 Token: 193 A O ABS\n",
      "Doc: 690 Token: 210 A O ABS\n",
      "Doc: 694 Token: 120 its O ABS\n",
      "Doc: 697 Token: 55 A ABS O\n",
      "Doc: 697 Token: 222 A O ABS\n",
      "Doc: 698 Token: 357 its O ABS\n",
      "Doc: 701 Token: 44 A ABS O\n",
      "Doc: 701 Token: 97 A O ABS\n",
      "Doc: 701 Token: 147 A O ABS\n",
      "Doc: 701 Token: 195 A O ABS\n",
      "Doc: 706 Token: 10 its O ABS\n",
      "Doc: 706 Token: 45 its O ABS\n",
      "Doc: 708 Token: 15 its O ABS\n",
      "Doc: 710 Token: 26 its O ABS\n",
      "Doc: 723 Token: 22 A ABS O\n",
      "Doc: 723 Token: 63 A O ABS\n",
      "Doc: 723 Token: 110 A O ABS\n",
      "Doc: 723 Token: 136 A O ABS\n",
      "Doc: 725 Token: 45 its O ABS\n",
      "Doc: 726 Token: 31 A ABS O\n",
      "Doc: 726 Token: 50 A O ABS\n",
      "Doc: 727 Token: 111 its O ABS\n",
      "Doc: 733 Token: 75 AS ABS O\n",
      "Doc: 733 Token: 188 AS ABS O\n",
      "Doc: 736 Token: 57 A ABS O\n",
      "Doc: 737 Token: 223 its O ABS\n",
      "Doc: 739 Token: 43 A ABS O\n",
      "Doc: 739 Token: 52 A O ABS\n",
      "Doc: 739 Token: 112 A O ABS\n",
      "Doc: 739 Token: 264 its O ABS\n",
      "Doc: 739 Token: 337 A O ABS\n",
      "Doc: 741 Token: 174 its O ABS\n",
      "Doc: 751 Token: 81 A ABS O\n",
      "Doc: 751 Token: 145 A ABS O\n",
      "Doc: 751 Token: 224 A ABS O\n",
      "Doc: 754 Token: 268 its O ABS\n",
      "Doc: 759 Token: 31 i ABS O\n",
      "Doc: 762 Token: 11 its O ABS\n",
      "Doc: 762 Token: 258 its O ABS\n",
      "Doc: 770 Token: 308 its O ABS\n",
      "Doc: 771 Token: 28 A ABS O\n",
      "Doc: 771 Token: 78 A O ABS\n",
      "Doc: 771 Token: 196 A O ABS\n",
      "Doc: 772 Token: 159 its O ABS\n",
      "Doc: 773 Token: 5 A O ABS\n",
      "Doc: 778 Token: 149 A ABS O\n",
      "Doc: 778 Token: 187 A ABS O\n",
      "Doc: 779 Token: 22 A ABS O\n",
      "Doc: 779 Token: 66 A ABS O\n",
      "Doc: 779 Token: 151 A ABS O\n",
      "Doc: 780 Token: 154 its O ABS\n",
      "Doc: 783 Token: 241 A O ABS\n",
      "Doc: 783 Token: 272 A ABS O\n",
      "194 97\n",
      "new: Counter({'its': 111, 'A': 49, 'WAS': 20, 'Its': 4, 'I': 4, 'AS': 3, 'WHO': 2, 'Am': 1})\n",
      "old: Counter({'A': 73, 'i': 8, 'I': 7, 'having': 3, 'NO': 2, 'AS': 2, 'Am': 1, 'WHO': 1})\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "TYPE = 'WISER_LABELS'\n",
    "LF = \"StopWords\"\n",
    "old_token_pool = Counter()\n",
    "new_token_pool = Counter()\n",
    "count_old_I = count_new_I = 0\n",
    "for i in range(len(new_ncbi_docs)):\n",
    "    try:\n",
    "        if new_ncbi_docs[i]['tokens'] == old_ncbi_docs[i]['tokens']:  # only five unmatched\n",
    "            count += 1\n",
    "            if new_ncbi_docs[i][TYPE][LF] != old_ncbi_docs[i][TYPE][LF]:\n",
    "                for j in range(len(new_ncbi_docs[i][TYPE][LF])):\n",
    "                    if new_ncbi_docs[i][TYPE][LF][j] != old_ncbi_docs[i][TYPE][LF][j]:\n",
    "                        print(f\"Doc: {i}\", f\"Token: {j}\", new_ncbi_docs[i]['tokens'][j], new_ncbi_docs[i][TYPE][LF][j], old_ncbi_docs[i][TYPE][LF][j])\n",
    "                        if new_ncbi_docs[i][TYPE][LF][j] == \"O\":\n",
    "                            count_new_I += 1\n",
    "                            new_token_pool[new_ncbi_docs[i]['tokens'][j]] += 1\n",
    "                        elif old_ncbi_docs[i][TYPE][LF][j] == \"O\":\n",
    "                            count_old_I += 1\n",
    "                            old_token_pool[new_ncbi_docs[i]['tokens'][j]] += 1\n",
    "    except:\n",
    "        print(i)\n",
    "\n",
    "print(count_new_I, count_old_I)\n",
    "print(\"new:\", new_token_pool)\n",
    "print(\"old:\", old_token_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Token: 0 genotype\nToken: 1 and\nToken: 2 phenotype\nToken: 3 in\nToken: 4 patient\nToken: 5 with\nToken: 6 dihydropyrimidine\nToken: 7 dehydrogenase\nToken: 8 deficiency\nToken: 9 .\nToken: 10 dihydropyrimidine\nToken: 11 dehydrogenase\nToken: 12 (\nToken: 13 DPD\nToken: 14 )\nToken: 15 deficiency\nToken: 16 be\nToken: 17 an\nToken: 18 autosomal\nToken: 19 recessive\nToken: 20 disease\nToken: 21 characterise\nToken: 22 by\nToken: 23 thymine\nToken: 24 -\nToken: 25 uraciluria\nToken: 26 in\nToken: 27 homozygous\nToken: 28 deficient\nToken: 29 patient\nToken: 30 and\nToken: 31 have\nToken: 32 be\nToken: 33 associate\nToken: 34 with\nToken: 35 a\nToken: 36 variable\nToken: 37 clinical\nToken: 38 phenotype\nToken: 39 .\nToken: 40 in\nToken: 41 order\nToken: 42 to\nToken: 43 understand\nToken: 44 the\nToken: 45 genetic\nToken: 46 and\nToken: 47 phenotypic\nToken: 48 basis\nToken: 49 for\nToken: 50 DPD\nToken: 51 deficiency\nToken: 52 ,\nToken: 53 we\nToken: 54 have\nToken: 55 review\nToken: 56 17\nToken: 57 family\nToken: 58 present\nToken: 59 22\nToken: 60 patient\nToken: 61 with\nToken: 62 complete\nToken: 63 deficiency\nToken: 64 of\nToken: 65 DPD\nToken: 66 .\nToken: 67 in\nToken: 68 this\nToken: 69 group\nToken: 70 of\nToken: 71 patient\nToken: 72 ,\nToken: 73 7\nToken: 74 different\nToken: 75 mutation\nToken: 76 have\nToken: 77 be\nToken: 78 identify\nToken: 79 ,\nToken: 80 include\nToken: 81 2\nToken: 82 deletion\nToken: 83 [\nToken: 84 295\nToken: 85 -\nToken: 86 298delTCAT\nToken: 87 ,\nToken: 88 1897delc\nToken: 89 ]\nToken: 90 ,\nToken: 91 1\nToken: 92 splice\nToken: 93 -\nToken: 94 site\nToken: 95 mutation\nToken: 96 [\nToken: 97 IVS14\nToken: 98 +\nToken: 99 1\nToken: 100 G\nToken: 101 >\nToken: 102 A\nToken: 103 )\nToken: 104 ]\nToken: 105 and\nToken: 106 4\nToken: 107 missense\nToken: 108 mutation\nToken: 109 (\nToken: 110 85\nToken: 111 t\nToken: 112 >\nToken: 113 C\nToken: 114 ,\nToken: 115 703C\nToken: 116 >\nToken: 117 T\nToken: 118 ,\nToken: 119 2658\nToken: 120 G\nToken: 121 >\nToken: 122 A\nToken: 123 ,\nToken: 124 2983\nToken: 125 G\nToken: 126 >\nToken: 127 T\nToken: 128 )\nToken: 129 .\nToken: 130 analysis\nToken: 131 of\nToken: 132 the\nToken: 133 prevalence\nToken: 134 of\nToken: 135 the\nToken: 136 various\nToken: 137 mutation\nToken: 138 among\nToken: 139 DPD\nToken: 140 patient\nToken: 141 have\nToken: 142 show\nToken: 143 that\nToken: 144 the\nToken: 145 G\nToken: 146 -\nToken: 147 -\nToken: 148 >\nToken: 149 a\nToken: 150 point\nToken: 151 mutation\nToken: 152 in\nToken: 153 the\nToken: 154 invariant\nToken: 155 splice\nToken: 156 donor\nToken: 157 site\nToken: 158 be\nToken: 159 by\nToken: 160 far\nToken: 161 the\nToken: 162 most\nToken: 163 common\nToken: 164 (\nToken: 165 52\nToken: 166 %\nToken: 167 )\nToken: 168 ,\nToken: 169 whereas\nToken: 170 the\nToken: 171 other\nToken: 172 six\nToken: 173 mutation\nToken: 174 be\nToken: 175 less\nToken: 176 frequently\nToken: 177 observe\nToken: 178 .\nToken: 179 a\nToken: 180 large\nToken: 181 phenotypic\nToken: 182 variability\nToken: 183 have\nToken: 184 be\nToken: 185 observe\nToken: 186 ,\nToken: 187 with\nToken: 188 convulsive\nToken: 189 disorder\nToken: 190 ,\nToken: 191 motor\nToken: 192 retardation\nToken: 193 and\nToken: 194 mental\nToken: 195 retardation\nToken: 196 be\nToken: 197 the\nToken: 198 most\nToken: 199 abundant\nToken: 200 manifestation\nToken: 201 .\nToken: 202 a\nToken: 203 clear\nToken: 204 correlation\nToken: 205 between\nToken: 206 the\nToken: 207 genotype\nToken: 208 and\nToken: 209 phenotype\nToken: 210 have\nToken: 211 not\nToken: 212 be\nToken: 213 establish\nToken: 214 .\nToken: 215 an\nToken: 216 altered\nToken: 217 beta\nToken: 218 -\nToken: 219 alanine\nToken: 220 ,\nToken: 221 uracil\nToken: 222 and\nToken: 223 thymine\nToken: 224 homeostasis\nToken: 225 might\nToken: 226 underlie\nToken: 227 the\nToken: 228 various\nToken: 229 clinical\nToken: 230 abnormality\nToken: 231 encounter\nToken: 232 in\nToken: 233 patient\nToken: 234 with\nToken: 235 DPD\nToken: 236 deficiency\nToken: 237 .\n['ABS', 'ABS', 'ABS', 'O', 'ABS', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'O', 'ABS', 'O', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'ABS', 'O', 'ABS', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'ABS', 'ABS', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'ABS', 'O', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'ABS', 'ABS', 'O', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'ABS', 'ABS', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'ABS', 'ABS', 'ABS', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'ABS', 'ABS', 'ABS', 'O', 'O', 'ABS', 'ABS', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'ABS', 'ABS', 'ABS', 'O', 'ABS', 'ABS', 'ABS', 'O', 'O', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'ABS', 'ABS', 'ABS', 'ABS', 'O', 'ABS', 'O', 'ABS', 'ABS', 'ABS']\n"
     ]
    }
   ],
   "source": [
    "from wiser.rules import TaggingRule\n",
    "stop_words = {\"a\", \"as\", \"be\", \"but\", \"do\", \"even\",\n",
    "              \"for\", \"from\",\n",
    "              \"had\", \"has\", \"have\", \"i\", \"in\", \"is\", \"its\", \"just\",\n",
    "              \"my\", \"no\", \"not\", \"on\", \"or\",\n",
    "              \"that\", \"the\", \"these\", \"this\", \"those\", \"to\", \"very\",\n",
    "              \"what\", \"which\", \"who\", \"with\"}\n",
    "class StopWords(TaggingRule):\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "        for i in range(len(instance['tokens'])):\n",
    "            print(f\"Token: {i}\", instance['tokens'][i].lemma_)\n",
    "            if instance['tokens'][i].lemma_ in stop_words:\n",
    "                labels[i] = 'O'\n",
    "        return labels\n",
    "\n",
    "\n",
    "lf = StopWords()\n",
    "print(lf.apply_instance(new_ncbi_docs_allen[11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wiser.rules import TaggingRule\n",
    "# class Deficiency(TaggingRule):\n",
    "\n",
    "#     def apply_instance(self, instance):\n",
    "#         labels = ['ABS'] * len(instance['tokens'])\n",
    "#         dep_ = [tok.dep_ for tok in instance['tokens']]\n",
    "#         lemma_ = [tok.lemma_ for tok in instance['tokens']]\n",
    "#         L = list(zip(lemma_, dep_))\n",
    "#         # print(list(zip(dep_, lemma_)))\n",
    "\n",
    "\n",
    "#         # \"___ deficiency\"\n",
    "#         for i in range(len(instance['tokens']) - 1):\n",
    "#             if instance['tokens'][i].dep_ == 'compound' and instance['tokens'][i +\n",
    "#                                                                                1].lemma_ == 'deficiency':\n",
    "#                 labels[i] = 'I'\n",
    "#                 labels[i + 1] = 'I'\n",
    "\n",
    "#                 # Adds any other compound tokens before the phrase\n",
    "#                 for j in range(i - 1, -1, -1):\n",
    "#                     if instance['tokens'][j].dep_ == 'compound':\n",
    "#                         labels[j] = 'I'\n",
    "#                     else:\n",
    "#                         break\n",
    "\n",
    "#         # \"deficiency of ___\"\n",
    "#         for i in range(len(instance['tokens']) - 2):\n",
    "#             if instance['tokens'][i].lemma_ == 'deficiency' and instance['tokens'][i + 1].lemma_ == 'of':\n",
    "#                 labels[i] = 'I'\n",
    "#                 labels[i + 1] = 'I'\n",
    "#                 nnp_active = False\n",
    "#                 for j in range(i + 2, len(instance['tokens'])):\n",
    "#                     if instance['tokens'][j].pos_ in ('NOUN', 'PROPN'):\n",
    "#                         if not nnp_active:\n",
    "#                             nnp_active = True\n",
    "#                     elif nnp_active:\n",
    "#                         break\n",
    "#                     labels[j] = 'I'\n",
    "        \n",
    "#         return L\n",
    "        \n",
    "\n",
    "\n",
    "# lf = Deficiency()\n",
    "# # print(lf.apply_instance(new_ncbi_docs_allen[19]))\n",
    "# for doc, token in log:\n",
    "#     L = lf.apply_instance(new_ncbi_docs_allen[doc])\n",
    "#     print(doc, L[token], L[token + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wiser.rules import TaggingRule\n",
    "# class CommonSuffixes(TaggingRule):\n",
    "\n",
    "#     suffixes = {\n",
    "#         \"agia\",\n",
    "#         \"cardia\",\n",
    "#         \"trophy\",\n",
    "#         \"toxic\",\n",
    "#         \"itis\",\n",
    "#         \"emia\",\n",
    "#         \"pathy\",\n",
    "#         \"plasia\"}\n",
    "\n",
    "#     def apply_instance(self, instance):\n",
    "#         labels = ['ABS'] * len(instance['tokens'])\n",
    "\n",
    "#         for i in range(len(instance['tokens'])):\n",
    "#             for suffix in self.suffixes:\n",
    "#                 if instance['tokens'][i].lemma_.endswith(suffix):\n",
    "#                     labels[i] = 'I'\n",
    "#             print(\"lemma:\", instance['tokens'][i].lemma_)\n",
    "#         return labels\n",
    "\n",
    "\n",
    "# lf = CommonSuffixes()\n",
    "# print(lf.apply_instance(new_ncbi_docs_allen[6]))\n",
    "# # print(\"‚úÖüèÉ‚Äç‚ôÇÔ∏è run LF_CommonSuffixes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('env': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "interpreter": {
   "hash": "f2133b35deaae558e57d87b0b3b55318d694d2ce778ad1feaabe6e4df441e1b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}